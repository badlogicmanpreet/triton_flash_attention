{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1548d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manpreet.singh/git/flash-attention/attnpy/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      "tensor([[1.0000, 2.0000, 0.5000, 3.0000],\n",
      "        [0.1000, 0.8000, 5.0000, 1.2000],\n",
      "        [3.0000, 2.0000, 1.0000, 0.5000]])\n",
      "\n",
      "After Softmax (row-wise):\n",
      "tensor([[0.0854, 0.2321, 0.0518, 0.6308],\n",
      "        [0.0071, 0.0144, 0.9571, 0.0214],\n",
      "        [0.6308, 0.2321, 0.0854, 0.0518]])\n",
      "\n",
      "Row sums (should all be 1):\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# create a simple attention matrix and perform softmax using PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a 3x4 matrix with some sample values\n",
    "matrix = torch.tensor([\n",
    "    [1.0, 2.0, 0.5, 3.0],  # First row\n",
    "    [0.1, 0.8, 5.0, 1.2],  # Second row\n",
    "    [3.0, 2.0, 1.0, 0.5]   # Third row\n",
    "])\n",
    "\n",
    "print(\"Original Matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "# Apply softmax row-wise (dim=1)\n",
    "softmax_result = F.softmax(matrix, dim=1)\n",
    "\n",
    "print(\"\\nAfter Softmax (row-wise):\")\n",
    "print(softmax_result)\n",
    "\n",
    "# Verify that each row sums to 1\n",
    "print(\"\\nRow sums (should all be 1):\")\n",
    "print(torch.sum(softmax_result, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023cb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "tensor([[1.0000, 2.0000, 0.5000, 3.0000],\n",
      "        [0.1000, 0.8000, 5.0000, 1.2000],\n",
      "        [3.0000, 2.0000, 1.0000, 0.5000]])\n",
      "\n",
      "Stable softmax result:\n",
      "tensor([[0.0854, 0.2321, 0.0518, 0.6308],\n",
      "        [0.0071, 0.0144, 0.9571, 0.0214],\n",
      "        [0.6308, 0.2321, 0.0854, 0.0518]])\n",
      "\n",
      "Row sums (should be 1):\n",
      "tensor([1.0000, 1.0000, 1.0000])\n",
      "\n",
      "PyTorch's built-in softmax:\n",
      "tensor([[0.0854, 0.2321, 0.0518, 0.6308],\n",
      "        [0.0071, 0.0144, 0.9571, 0.0214],\n",
      "        [0.6308, 0.2321, 0.0854, 0.0518]])\n",
      "\n",
      "Difference:\n",
      "tensor(1.1921e-07)\n"
     ]
    }
   ],
   "source": [
    "# create a simple attention matrix and perform softmax using custom implementation which is fused\n",
    "\n",
    "import torch\n",
    "\n",
    "def stable_softmax(x):\n",
    "    # Get batch size and sequence length\n",
    "    batch_size = x.shape[0]\n",
    "    seq_len = x.shape[1]\n",
    "    \n",
    "    # Initialize arrays to hold intermediate values\n",
    "    m = torch.full((batch_size,), float('-inf'), device=x.device)\n",
    "    l = torch.zeros(batch_size, device=x.device)\n",
    "    \n",
    "    # First loop: compute maximum and sum for each row\n",
    "    for i in range(seq_len):\n",
    "        column = x[:, i]  # Get current column for all batches\n",
    "        \n",
    "        # Update maximum values\n",
    "        prev_m = m.clone()\n",
    "        m = torch.maximum(prev_m, column)\n",
    "        \n",
    "        # Update sum with scaling\n",
    "        scale_factor = torch.exp(prev_m - m)\n",
    "        update = torch.exp(column - m)\n",
    "        l = l * scale_factor + update\n",
    "    \n",
    "    # Second loop: compute final softmax values\n",
    "    result = torch.zeros_like(x)\n",
    "    for k in range(seq_len):\n",
    "        result[:, k] = torch.exp(x[:, k] - m) / l\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Test with the example matrix\n",
    "matrix = torch.tensor([\n",
    "    [1.0, 2.0, 0.5, 3.0],\n",
    "    [0.1, 0.8, 5.0, 1.2],\n",
    "    [3.0, 2.0, 1.0, 0.5]\n",
    "])\n",
    "\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "print(\"\\nStable softmax result:\")\n",
    "result = stable_softmax(matrix)\n",
    "print(result)\n",
    "\n",
    "print(\"\\nRow sums (should be 1):\")\n",
    "print(torch.sum(result, dim=1))\n",
    "\n",
    "# Compare with PyTorch's softmax\n",
    "import torch.nn.functional as F\n",
    "print(\"\\nPyTorch's built-in softmax:\")\n",
    "torch_result = F.softmax(matrix, dim=1)\n",
    "print(torch_result)\n",
    "\n",
    "print(\"\\nDifference:\")\n",
    "print(torch.abs(result - torch_result).max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attnpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
